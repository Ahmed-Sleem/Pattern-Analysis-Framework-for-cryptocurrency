{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1KF1o60g2nSE",
        "IMHsrHyP2x3K",
        "EjRPSkMxDQP4",
        "wsgp9LFk_3ue",
        "z09dEPdnHrgz",
        "PQJtrdTUBWWS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#pre-run"
      ],
      "metadata": {
        "id": "1KF1o60g2nSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tslearn\n",
        "!pip install mplfinance\n",
        "!pip install dtw\n",
        "!pip install pandas-ta\n",
        "!pip install numba\n",
        "!pip install tqdm\n",
        "!pip install Backtesting\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mplfinance as mpf\n",
        "import math\n",
        "from io import StringIO\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "from joblib import load\n",
        "import csv\n",
        "import shutil\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import glob\n",
        "import re\n",
        "import pandas_ta as ta\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.neighbors import KDTree\n",
        "import ast\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6sNbPZKq2mme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20ee537-89fb-4ca7-d98b-be3e2b98bddc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tslearn in /usr/local/lib/python3.10/dist-packages (0.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from tslearn) (0.58.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.4.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (0.41.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tslearn) (3.4.0)\n",
            "Requirement already satisfied: mplfinance in /usr/local/lib/python3.10/dist-packages (0.12.10b0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mplfinance) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mplfinance) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mplfinance) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mplfinance) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.16.0)\n",
            "Requirement already satisfied: dtw in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dtw) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dtw) (1.11.4)\n",
            "Requirement already satisfied: pandas-ta in /usr/local/lib/python3.10/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas-ta) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.16.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: Backtesting in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from Backtesting) (1.25.2)\n",
            "Requirement already satisfied: pandas!=0.25.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from Backtesting) (2.0.3)\n",
            "Requirement already satisfied: bokeh>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from Backtesting) (3.3.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (1.2.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (24.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.4.0->Backtesting) (2024.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=0.25.0,>=0.25.0->Backtesting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=0.25.0,>=0.25.0->Backtesting) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=0.25.0,>=0.25.0->Backtesting) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh>=1.4.0->Backtesting) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=0.25.0,>=0.25.0->Backtesting) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main"
      ],
      "metadata": {
        "id": "IMHsrHyP2x3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "\n",
        "\n",
        "\n",
        "#preprocess data to points\n",
        "\n",
        "def pips(data: np.array, n_pips: int, dist_measure: int):\n",
        "\n",
        "    # dist_measure\n",
        "    # 1 = Euclidean Distance\n",
        "    # 2 = Perpindicular Distance\n",
        "    # 3 = Vertical Distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #first phase converting\n",
        "    n_pips = n_pips + 2\n",
        "\n",
        "\n",
        "    pips_x = [0, len(data) - 1]  # Index\n",
        "    pips_y = [data[0], data[-1]] # Price\n",
        "\n",
        "    for curr_point in range(2, n_pips):\n",
        "\n",
        "        md = 0.0 # Max distance\n",
        "        md_i = -1 # Max distance index\n",
        "        insert_index = -1\n",
        "\n",
        "        for k in range(0, curr_point - 1):\n",
        "\n",
        "            # Left adjacent, right adjacent indices\n",
        "            left_adj = k\n",
        "            right_adj = k + 1\n",
        "\n",
        "            time_diff = pips_x[right_adj] - pips_x[left_adj]\n",
        "            price_diff = pips_y[right_adj] - pips_y[left_adj]\n",
        "            slope = price_diff / time_diff\n",
        "            intercept = pips_y[left_adj] - pips_x[left_adj] * slope;\n",
        "\n",
        "            for i in range(pips_x[left_adj] + 1, pips_x[right_adj]):\n",
        "\n",
        "                d = 0.0 # Distance\n",
        "                if dist_measure == 1: # Euclidean distance\n",
        "                    d =  ( (pips_x[left_adj] - i) ** 2 + (pips_y[left_adj] - data[i]) ** 2 ) ** 0.5\n",
        "                    d += ( (pips_x[right_adj] - i) ** 2 + (pips_y[right_adj] - data[i]) ** 2 ) ** 0.5\n",
        "                elif dist_measure == 2: # Perpindicular distance\n",
        "                    d = abs( (slope * i + intercept) - data[i] ) / (slope ** 2 + 1) ** 0.5\n",
        "                else: # Vertical distance\n",
        "                    d = abs( (slope * i + intercept) - data[i] )\n",
        "\n",
        "                if d > md:\n",
        "                    md = d\n",
        "                    md_i = i\n",
        "                    insert_index = right_adj\n",
        "\n",
        "        pips_x.insert(insert_index, md_i)\n",
        "        pips_y.insert(insert_index, data[md_i])\n",
        "\n",
        "\n",
        "\n",
        "     #second phase converting\n",
        "    points_no = len(pips_x)  #number of points\n",
        "    points = [] #list of new converting data\n",
        "\n",
        "\n",
        "    #the rate of change of y over the rate of change of x for every point\n",
        "    for i in range(2,points_no) :\n",
        "\n",
        "      x = ((pips_x[i] - pips_x[i-1])/ pips_x[i-1] ) * 100\n",
        "      y = ((pips_y[i] - pips_y[i-1])/ pips_y[i-1] ) * 100\n",
        "\n",
        "      z = y / x\n",
        "\n",
        "      points.append(z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "\n",
        "#creating points dataset\n",
        "def create_dataset(csv_file,no_of_points_in_raw,window_size=24,step=5,name=\"dataset.csv\"):\n",
        "\n",
        "    #load data\n",
        "    data = pd.read_csv(csv_file)\n",
        "    data['date'] = data['date'].astype('datetime64[s]')\n",
        "    data = data.set_index('date')\n",
        "\n",
        "\n",
        "    #dataset\n",
        "    x = data['close'].to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "    #create dataset\n",
        "    dataset = pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Iterate through the data\n",
        "    for i in range(0, len(x) - window_size + 1, step):\n",
        "        window_data = x[i:i+window_size]\n",
        "\n",
        "\n",
        "        #get points\n",
        "        points = pips(window_data, no_of_points_in_raw, 2)\n",
        "\n",
        "\n",
        "        #creating the new record\n",
        "        new_record = {\n",
        "            'points': points\n",
        "\n",
        "        }\n",
        "\n",
        "        # Convert the new record to a DataFrame\n",
        "        new_df = pd.DataFrame([new_record])\n",
        "\n",
        "        # Concatenate the existing DataFrame with the new record DataFrame\n",
        "        dataset = pd.concat([dataset, new_df], ignore_index=True)\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    dataset.to_csv(name, index=False)\n",
        "\n",
        "    #return\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def generate_unique_random_numbers(start, end, count):\n",
        "    if end - start + 1 < count:\n",
        "        raise ValueError(\"Count must be less than or equal to the range of numbers.\")\n",
        "    return random.sample(range(start, end + 1), count)\n",
        "\n",
        "\n",
        "#clustering and saving clusters data\n",
        "def cluster_and_save(dataset, n_clusters,folder=\"clusters\"):\n",
        "    # Preprocess data (scaling)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "    # Apply K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "    # Create a folder to save clusters if it doesn't exist\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # Convert numpy array to DataFrame\n",
        "    dataset_df = pd.DataFrame(dataset, columns=[f'Feature_{i+1}' for i in range(dataset.shape[1])])\n",
        "\n",
        "    # Iterate over each cluster\n",
        "    for cluster_num in range(n_clusters):\n",
        "        # Filter data points belonging to the current cluster\n",
        "        cluster_data = dataset_df[cluster_labels == cluster_num]\n",
        "\n",
        "        # Convert each row to a list and save as a single row containing a list of features\n",
        "        cluster_data_list = cluster_data.values.tolist()\n",
        "\n",
        "        #id = generate_unique_random_numbers(0,999,cluster_num+1)\n",
        "        # Save the cluster data to a file\n",
        "        #file_name = f'cluster_{cluster_num + 1}_{id}.csv'\n",
        "\n",
        "        # Save the cluster data to a file\n",
        "        file_name = f'cluster_{cluster_num + 1}.csv'\n",
        "        file_path = os.path.join(folder, file_name)\n",
        "        with open(file_path, 'w') as file:\n",
        "            for row in cluster_data_list:\n",
        "                file.write(','.join(map(str, row)) + '\\n')\n",
        "        print(f\"Cluster {cluster_num + 1} saved to: {file_path}\")\n",
        "\n",
        "\n",
        "#determine the best number of clusters\n",
        "def determine_optimal_clusters(dataset, cluster_range):\n",
        "    # Preprocess data (scaling)\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "    # Initialize variables to store silhouette scores\n",
        "    silhouette_scores = []\n",
        "\n",
        "    # Iterate over different number of clusters\n",
        "    for n_clusters in cluster_range:\n",
        "        # Apply K-means clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Find the number of clusters with the highest silhouette score\n",
        "    optimal_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "    return optimal_clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#full clustering sequence\n",
        "def clustering(data_file,row_points,window,step,range_clusters=0,no_of_clusters=0,name_dataset=\"dataset.csv\",folder=\"clusters\"):\n",
        "\n",
        "  #preprocessing\n",
        "  create_dataset(data_file,row_points,window,step,name_dataset)\n",
        "\n",
        "  print(\"preprocessing done !\")\n",
        "\n",
        "  # Load dataset from CSV\n",
        "  dataset = pd.read_csv(name_dataset)\n",
        "\n",
        "  # Convert string representation of points to lists of floats\n",
        "  dataset['points'] = dataset['points'].apply(eval)\n",
        "\n",
        "  # Convert points to numpy array\n",
        "  points_array = np.array(dataset['points'].tolist())\n",
        "\n",
        "  print(\"load points done !\")\n",
        "\n",
        "\n",
        "  if no_of_clusters == 0 :\n",
        "\n",
        "    # Load dataset from CSV\n",
        "    dataset = pd.read_csv(name_dataset)\n",
        "\n",
        "    # Convert string representation of points to lists of floats\n",
        "    dataset['points'] = dataset['points'].apply(eval)\n",
        "\n",
        "\n",
        "\n",
        "    # Convert points to numpy array\n",
        "    points_array = np.array(dataset['points'].tolist())\n",
        "\n",
        "\n",
        "\n",
        "    # Define range of cluster numbers to test\n",
        "    cluster_range = range_clusters\n",
        "\n",
        "    # Determine the optimal number of clusters using the Silhouette Method\n",
        "    optimal_clusters = determine_optimal_clusters(points_array, cluster_range)\n",
        "\n",
        "    print(\"Optimal number of clusters:\", optimal_clusters)\n",
        "\n",
        "\n",
        "\n",
        "    # Define the number of clusters\n",
        "    n_clusters = int(optimal_clusters)\n",
        "\n",
        "\n",
        "  elif no_of_clusters != 0 :\n",
        "    print(\"start clustering to the chosen clusters number\")\n",
        "    n_clusters = no_of_clusters\n",
        "\n",
        "  else:\n",
        "    print(\"error please choose a range or a number of cluster to start clustering !\")\n",
        "\n",
        "  print(\"start clustering\")\n",
        "\n",
        "  # Perform clustering and save clusters\n",
        "  cluster_and_save(points_array, n_clusters,folder)\n",
        "\n",
        "  print(\"clustering done !\")\n",
        "\n",
        "\n",
        "#to plot clusters and visualize patterns\n",
        "def plot_clusters(cluster_folder='clusters',points=5):\n",
        "    # Get list of cluster files\n",
        "    cluster_files = [file for file in os.listdir(cluster_folder) if file.startswith('cluster_')]\n",
        "\n",
        "    # Plot each cluster separately\n",
        "    for file in cluster_files:\n",
        "        # Read cluster data from CSV\n",
        "        cluster_data = pd.read_csv(os.path.join(cluster_folder, file))\n",
        "\n",
        "        # Plot cluster data\n",
        "        plt.figure(figsize=(10, points+1))\n",
        "        for i in range(len(cluster_data)):\n",
        "            plt.plot(cluster_data.iloc[i].values, label=f'Record {i+1}')  # Plot each record separately\n",
        "\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('Feature Value')\n",
        "        plt.title(f'Cluster {file[:-4]} Records Overlay Plot')\n",
        "\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#to save plots\n",
        "\n",
        "def save_cluster_plots(cluster_folder='clusters', points=5, save_folder='cluster_plots'):\n",
        "\n",
        "\n",
        "\n",
        "    #clear old images\n",
        "    clear_clusters(save_folder)\n",
        "\n",
        "\n",
        "    if not os.path.isdir(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    # Get list of cluster files\n",
        "    cluster_files = [file for file in os.listdir(cluster_folder) if file.startswith('cluster_')]\n",
        "\n",
        "    # Plot each cluster separately\n",
        "    for file in cluster_files:\n",
        "        # Read cluster data from CSV\n",
        "        cluster_data = pd.read_csv(os.path.join(cluster_folder, file))\n",
        "\n",
        "        # Plot cluster data\n",
        "        plt.figure(figsize=(10, points+1))\n",
        "        for i in range(len(cluster_data)):\n",
        "            plt.plot(cluster_data.iloc[i].values, label=f'Record {i+1}')  # Plot each record separately\n",
        "\n",
        "        plt.xlabel('Feature Index')\n",
        "        plt.ylabel('Feature Value')\n",
        "        plt.title(f'Cluster {file[:-4]} Records Overlay Plot')\n",
        "\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.savefig(os.path.join(save_folder, f'cluster_{file[:-4]}.png'))  # Save plot as PNG file\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_clusters(folder_path, no_of_clusters):\n",
        "    \"\"\"Load cluster data from CSV files in the specified folder.\"\"\"\n",
        "    cluster_data = []\n",
        "    # Get a list of files in the folder\n",
        "    files = os.listdir(folder_path)\n",
        "    # Iterate over the files\n",
        "    for file_name in files:\n",
        "        # Check if the file is a CSV file and matches the cluster naming convention\n",
        "        if file_name.endswith('.csv') and file_name.startswith('cluster_'):\n",
        "            # Extract the cluster number from the file name\n",
        "            cluster_number = int(file_name.split('_')[1].split('.')[0])\n",
        "            # Check if the cluster number is within the specified range\n",
        "            if cluster_number <= no_of_clusters:\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                cluster_df = pd.read_csv(file_path)\n",
        "                cluster_data.append(cluster_df)\n",
        "    return cluster_data\n",
        "\n",
        "\n",
        "\n",
        "#count clusters\n",
        "\n",
        "def count_cluster(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return \"Invalid folder path\"\n",
        "\n",
        "    file_count = 0\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.startswith(\"cluster_\") and file.endswith(\".csv\"):\n",
        "            file_count += 1\n",
        "\n",
        "\n",
        "    print(f'number of cluster = {file_count}')\n",
        "    return file_count\n",
        "\n",
        "\n",
        "#delete the empty clusters\n",
        "\n",
        "def delete_empty(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return \"Invalid folder path\"\n",
        "\n",
        "    files_to_rename = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            with open(file_path, 'r') as csv_file:\n",
        "                csv_reader = csv.reader(csv_file)\n",
        "                num_records = sum(1 for _ in csv_reader)  # Count the number of records\n",
        "                if num_records < 2:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted {file} due to less than 2 records.\")\n",
        "                else:\n",
        "                    files_to_rename.append(file)\n",
        "\n",
        "\n",
        "#to organize if needed\n",
        "\n",
        "def organize(folder_path=\"clusters\"):\n",
        "  data = os.path.abspath(folder_path)\n",
        "  for i, f in enumerate(os.listdir(data)):\n",
        "      src = os.path.join(data, f)\n",
        "      dst = os.path.join(data, (f\"cluster_{str(i + 1)}.csv\"))\n",
        "      os.rename(src, dst)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#to delete empty\n",
        "\n",
        "def clear_clusters(folder_path=\"clusters\"):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(\"Folder does not exist.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)  # Delete file\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)  # Delete directory and its contents\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#to download plots directly\n",
        "\n",
        "def download_plots(folder=\"cluster_plots\"):\n",
        "  files.download(f\"{folder}\")\n",
        "\n",
        "\n",
        "\n",
        "#to get elements list in some folder\n",
        "\n",
        "def list_files_in_folder(folder_path):\n",
        "    files_list = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if os.path.isfile(os.path.join(folder_path, file)):\n",
        "            files_list.append(file)\n",
        "    return files_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#second stage clustering\n",
        "def secondary_clusering (main_clusters_folder=\"clusters\",folder_name= \"secondary clusters\",clusters_range=range(1,30) ):\n",
        "\n",
        "  main_clusters = list_files_in_folder(main_clusters_folder)\n",
        "\n",
        "  for i in main_clusters :\n",
        "\n",
        "\n",
        "    data = i\n",
        "\n",
        "\n",
        "    n_clusters = determine_optimal_clusters(data,clusters_range)\n",
        "\n",
        "    print(f\"number of clusters : {n_clusters} for data : {data}\")\n",
        "\n",
        "    #clustering\n",
        "    cluster_and_save(data,n_clusters,folder_name)\n",
        "\n",
        "    print(f\"clustering done for : {data}\")\n",
        "\n",
        "    #cleaning empty clusters\n",
        "    delete_empty(folder_name)\n",
        "\n",
        "\n",
        "    #get the number of clusters after cleaning\n",
        "    clusters = count_cluster(folder_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dtermine clusters number for second stage\n",
        "def determine_optimal_clusters2(dataset, cluster_range):\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using silhouette score.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): Input data for clustering.\n",
        "        cluster_range (range): Range of number of clusters to consider.\n",
        "\n",
        "    Returns:\n",
        "        int: Optimal number of clusters.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    silhouette_scores = []\n",
        "\n",
        "    for n_clusters in cluster_range:\n",
        "        if n_clusters < len(dataset):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "            silhouette_avg = silhouette_score(scaled_data, cluster_labels)\n",
        "            silhouette_scores.append(silhouette_avg)\n",
        "        else:\n",
        "            silhouette_scores.append(-1)  # Flag indicating the number of clusters is too large\n",
        "\n",
        "    optimal_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
        "    return optimal_clusters if optimal_clusters < len(dataset) else len(dataset) - 1\n",
        "\n",
        "\n",
        "#saving clusters in second stage\n",
        "def cluster_and_save2(dataset, n_clusters, folder=\"clusters\", file_name=\"\",row_points=5):\n",
        "    \"\"\"\n",
        "    Cluster the data and save clusters to CSV files.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): Input data for clustering.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        folder (str): Folder path to save the cluster CSV files.\n",
        "        file_name (str): Name of the input file.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    dataset_df = pd.DataFrame(dataset)\n",
        "\n",
        "    for cluster_num in range(n_clusters):\n",
        "        cluster_data = dataset_df[cluster_labels == cluster_num]\n",
        "        cluster_data_list = cluster_data.values.tolist()\n",
        "        file_name_out = f\"{folder}/{os.path.basename(file_name)}_cluster_{cluster_num + 1}.csv\"\n",
        "        with open(file_name_out, 'w') as file:\n",
        "            for row in cluster_data_list:\n",
        "                file.write(','.join(map(str, row)) + '\\n')\n",
        "        print(f\"Cluster {cluster_num + 1} saved to: {file_name_out}\")\n",
        "\n",
        "\n",
        "#second stage clustering\n",
        "def secondary_clustering(main_clusters_folder=\"clusters\", secondary_clusters_folder=\"secondary_clusters\", clusters_range=range(1, 30),row_points=5):\n",
        "    \"\"\"\n",
        "    Perform secondary clustering on files in a folder.\n",
        "\n",
        "    Parameters:\n",
        "        main_clusters_folder (str): Path to the folder containing main cluster files.\n",
        "        secondary_clusters_folder (str): Path to the folder to save secondary cluster files.\n",
        "        clusters_range (range): Range of number of clusters to consider.\n",
        "    \"\"\"\n",
        "    main_clusters = os.listdir(main_clusters_folder)\n",
        "\n",
        "    for file_name in main_clusters:\n",
        "        file_path = os.path.join(main_clusters_folder, file_name)\n",
        "        data = pd.read_csv(file_path, header=None)\n",
        "\n",
        "\n",
        "        length_data = len(data)\n",
        "\n",
        "\n",
        "        print(f\"Data shape for file {file_name}: {data.shape}\")\n",
        "        print(f\"Data content for file {file_name}:\\n{data.head()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # dynamic range\n",
        "        if length_data > 500 :\n",
        "          print(\"data length is big , dynamic range activated\")\n",
        "          clusters_range = range(clusters_range.start+5,clusters_range.stop+10)\n",
        "        elif length_data > 1500 :\n",
        "          clusters_range = range(clusters_range.start+10,clusters_range.stop+20)\n",
        "        elif length_data > 2500 :\n",
        "          clusters_range = range(clusters_range.start+20,clusters_range.stop+20)\n",
        "        elif length_data > 3500 :\n",
        "          clusters_range = range(clusters_range.start+30,clusters_range.stop+30)\n",
        "\n",
        "        print(f\"clusters number range : {clusters_range}\")\n",
        "\n",
        "\n",
        "        n_clusters = determine_optimal_clusters2(data, clusters_range)\n",
        "        print(f\"Optimal number of clusters: {n_clusters} for data: {file_name}\")\n",
        "\n",
        "\n",
        "\n",
        "        if n_clusters < data.shape[0]:\n",
        "            cluster_and_save2(data, n_clusters, secondary_clusters_folder, file_name,row_points)\n",
        "            print(f\"Clustering done for: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Error: Number of clusters is equal to or greater than the number of samples.\")\n",
        "\n",
        "\n",
        "#combine several clusters in dataset\n",
        "def combine_secondary_clusters(folder_path, output_file):\n",
        "    \"\"\"\n",
        "    Combine secondary cluster CSV files into a single dataframe.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): Path to the folder containing secondary cluster CSV files.\n",
        "        output_file (str): Name of the output CSV file.\n",
        "    \"\"\"\n",
        "    # List to hold dataframes\n",
        "    dfs = []\n",
        "\n",
        "    # Iterate over each file in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "\n",
        "            # Extract cluster numbers from the file name using regular expressions\n",
        "            numbers = re.findall(r'\\d+', filename)\n",
        "            cluster_number = '_'.join(numbers)\n",
        "\n",
        "            # Read CSV file into a dataframe\n",
        "            df = pd.read_csv(os.path.join(folder_path, filename), header=None)\n",
        "\n",
        "            # Add a new column to identify the cluster\n",
        "            df['cluster'] = cluster_number\n",
        "\n",
        "            # Append the dataframe to the list\n",
        "            dfs.append(df)\n",
        "\n",
        "        else:\n",
        "\n",
        "          print(f\"error in file {filename}\")\n",
        "\n",
        "\n",
        "    # Concatenate all dataframes into a single dataframe\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Reorder columns to have the Cluster column as the last column\n",
        "    cols = combined_df.columns.tolist()\n",
        "    cols.remove('cluster')\n",
        "    cols.append('cluster')\n",
        "    combined_df = combined_df[cols]\n",
        "\n",
        "    # Save the combined dataframe to a new CSV file\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "    # Print the first few rows of the combined dataframe\n",
        "    #print(combined_df.head())\n",
        "\n",
        "\n",
        "\n",
        "#detection using algorithm\n",
        "#calculate clusters\n",
        "\n",
        "def calculate_unique_clusters(data_file):\n",
        "    # Load the data from CSV\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Extract clusters\n",
        "    clusters = data['cluster'].values\n",
        "\n",
        "    # Get the unique clusters present in the data\n",
        "    unique_clusters = np.unique(clusters)\n",
        "\n",
        "    return len(unique_clusters)\n",
        "\n",
        "\n",
        "#full clustering\n",
        "def pattern_clustering (data_file,row_points,window_size,step_size,range_main,range_secondary):\n",
        "\n",
        "  #clear old clusters\n",
        "  clear_clusters(\"clusters\")\n",
        "\n",
        "  #clustering\n",
        "  clustering(data_file ,row_points ,window_size ,step_size ,range_main )\n",
        "\n",
        "  #cleaning empty clusters\n",
        "  delete_empty(\"clusters\")\n",
        "\n",
        "\n",
        "  #get the number of clusters after cleaning\n",
        "  num_main_clusters = count_cluster(\"clusters\")\n",
        "\n",
        "\n",
        "  #clear old clusters\n",
        "  clear_clusters(\"secondary_clusters\")\n",
        "\n",
        "  #making secondary clusters:\n",
        "  secondary_clustering(\"clusters\", \"secondary_clusters\", range_secondary,row_points)\n",
        "\n",
        "\n",
        "  #cleaning empty clusters\n",
        "  delete_empty(\"secondary_clusters\")\n",
        "\n",
        "\n",
        "\n",
        "  #get the number of clusters after cleaning\n",
        "  num_secondary_clusters = count_cluster(\"secondary_clusters\")\n",
        "\n",
        "\n",
        "  print(f\"\"\"\n",
        "\n",
        "  number of main cluster : {num_main_clusters} \\n\n",
        "  number of secondary cluster : {num_secondary_clusters}\n",
        "\n",
        "\n",
        "  \"\"\")\n",
        "\n",
        "  return num_secondary_clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#to delete files\n",
        "def delete(file_path):\n",
        "\n",
        "  # Check if the file exists before deleting\n",
        "  if os.path.exists(file_path):\n",
        "      # Delete the file\n",
        "      os.remove(file_path)\n",
        "      print(f\"File {file_path} deleted successfully.\")\n",
        "  else:\n",
        "      print(f\"File {file_path} does not exist.\")\n",
        "\n",
        "\n",
        "\n",
        "#for analysing clusters behaviors\n",
        "def create_future_dataset (crypto_data_file,finder,num):\n",
        "\n",
        "  # Load crypto data\n",
        "  df = pd.read_csv(crypto_data_file)\n",
        "  data = df[\"close\"]\n",
        "\n",
        "\n",
        "\n",
        "  # Load or initialize last index processed\n",
        "  try:\n",
        "      with open('last_index.txt', 'r') as file:\n",
        "          last_index = int(file.read())\n",
        "  except FileNotFoundError:\n",
        "      last_index = 24  # Start from the beginning if last_index.txt does not exist\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize or load result DataFrame\n",
        "  try:\n",
        "      result = pd.read_csv('future_result.csv')\n",
        "  except FileNotFoundError:\n",
        "      result = pd.DataFrame()\n",
        "\n",
        "\n",
        "  long = range(last_index, len(data))\n",
        "\n",
        "  # Start processing from the last index\n",
        "  for i in long:\n",
        "      # Window\n",
        "      window = data[i - 24:i]\n",
        "      window = np.array(window).tolist()\n",
        "\n",
        "\n",
        "      # Next data\n",
        "      next_data = data[i:i + 25]\n",
        "      next_data = np.array(next_data).tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Get cluster\n",
        "      cluster = finder.find_cluster_for_record(window)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # New record to the dataframe\n",
        "      new_records = {\n",
        "          'window': [window],\n",
        "          'cluster': [cluster],\n",
        "          'next data': [next_data]\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "      result = pd.concat([result, pd.DataFrame(new_records)], ignore_index=True)\n",
        "\n",
        "      # Save last index processed\n",
        "      with open('last_index.txt', 'w') as file:\n",
        "          file.write(str(i))\n",
        "\n",
        "      # Save progress after processing each record\n",
        "      result.to_csv('future_result.csv', index=False)\n",
        "\n",
        "      # Print loading bar\n",
        "      #print_loading_bar(i + 1, len(long),103)\n",
        "\n",
        "\n",
        "  delete('last_index.txt')\n",
        "  print(\"Processing complete, future result created\")\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "#to create analysis data frame\n",
        "def create_clusters_analysis(future_dataset,pom=-1,window=24):\n",
        "\n",
        "    #the point of measuring\n",
        "    if pom == -1 :\n",
        "      change = pom\n",
        "    elif pom > window :\n",
        "      chnage = -1\n",
        "    else :\n",
        "      change = pom - 1\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    if isinstance(future_dataset, pd.DataFrame):\n",
        "        df = future_dataset\n",
        "    else:\n",
        "        df = pd.read_csv(future_dataset)\n",
        "\n",
        "    # Initialize dictionaries to store calculations for each cluster\n",
        "    cluster_analysis = {}\n",
        "\n",
        "    # Iterate over unique clusters and calculate analysis\n",
        "    for cluster_id, group in df.groupby('cluster'):\n",
        "        window_changes = []\n",
        "        next_data_changes = []\n",
        "\n",
        "        # Iterate over rows in the group\n",
        "        for index, row in group.iterrows():\n",
        "            # Convert string representations of lists to actual lists\n",
        "            window = ast.literal_eval(row['window'])\n",
        "            next_data = ast.literal_eval(row['next data'])\n",
        "\n",
        "            # Check if window and next_data arrays are not empty\n",
        "            if len(window) > 0 and len(next_data) > 0:\n",
        "\n",
        "\n",
        "                if len(window) > change:\n",
        "                  # Calculate changes in window records\n",
        "                  window_change = window[change] - window[0]\n",
        "                  window_changes.append(window_change)\n",
        "                else :\n",
        "                  # Calculate changes in window records\n",
        "                  window_change = window[-1] - window[0]\n",
        "                  window_changes.append(window_change)\n",
        "\n",
        "\n",
        "                if len(next_data) > change:\n",
        "                  # Calculate changes in next data\n",
        "                  next_data_change = next_data[change] - next_data[0]\n",
        "                  next_data_changes.append(next_data_change)\n",
        "                else :\n",
        "                  # Calculate changes in next data\n",
        "                  next_data_change = next_data[-1] - next_data[0]\n",
        "                  next_data_changes.append(next_data_change)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if len(window_changes) > 0 and len(next_data_changes) > 0:\n",
        "            # Calculate required statistics for window records changes\n",
        "            avg_window_change = np.mean(window_changes)\n",
        "            pos_window_percentage = (sum(1 for change in window_changes if change > 0) / len(window_changes)) * 100\n",
        "            neg_window_percentage = (sum(1 for change in window_changes if change < 0) / len(window_changes)) * 100\n",
        "            max_window_change = max(window_changes)\n",
        "            min_window_change = min(window_changes)\n",
        "\n",
        "            # Calculate required statistics for next data changes\n",
        "            avg_next_data_change = np.mean(next_data_changes)\n",
        "            pos_next_data_percentage = (sum(1 for change in next_data_changes if change > 0) / len(next_data_changes)) * 100\n",
        "            neg_next_data_percentage = (sum(1 for change in next_data_changes if change < 0) / len(next_data_changes)) * 100\n",
        "            max_next_data_change = max(next_data_changes)\n",
        "            min_next_data_change = min(next_data_changes)\n",
        "\n",
        "            # Calculate percentage of appearance of the cluster\n",
        "            appearance_percentage = (len(group) / len(df)) * 100\n",
        "\n",
        "            # Store analysis in dictionary\n",
        "            cluster_analysis[cluster_id] = {\n",
        "                'Average Change in Window': avg_window_change,\n",
        "                'Average Change in Next Data': avg_next_data_change,\n",
        "                'Positive Change Percentage in Window': pos_window_percentage,\n",
        "                'Positive Change Percentage in Next Data': pos_next_data_percentage,\n",
        "                'Negative Change Percentage in Window': neg_window_percentage,\n",
        "                'Negative Change Percentage in Next Data': neg_next_data_percentage,\n",
        "                'Max Change in Window': max_window_change,\n",
        "                'Max Change in Next Data': max_next_data_change,\n",
        "                'Min Change in Window': min_window_change,\n",
        "                'Min Change in Next Data': min_next_data_change,\n",
        "                'Appearance Percentage': appearance_percentage\n",
        "            }\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    df_analysis = pd.DataFrame.from_dict(cluster_analysis, orient='index')\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df_analysis.to_csv('clusters_analysis.csv', index_label='cluster')\n",
        "\n",
        "    print(\"Clusters analysis created successfully\")\n",
        "\n",
        "    return df_analysis\n",
        "\n",
        "\n",
        "\n",
        "#to get behaiors data frame\n",
        "def get_clusters_behavior(threshold,average, analysis_file):\n",
        "\n",
        "     # Read the CSV file into a DataFrame\n",
        "    if isinstance(analysis_file, pd.DataFrame):\n",
        "        df = analysis_file\n",
        "    else:\n",
        "        df = pd.read_csv(analysis_file, index_col='cluster')\n",
        "\n",
        "\n",
        "    # Initialize lists to store cluster behaviors\n",
        "    window_behavior = []\n",
        "    next_data_behavior = []\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "\n",
        "        # Determine behavior in window based on threshold\n",
        "        if row['Positive Change Percentage in Window'] > threshold and row['Average Change in Window'] > average:\n",
        "            window_behavior.append('Up Trend')\n",
        "        elif row['Negative Change Percentage in Window'] > threshold and row['Average Change in Window'] < (- average) :\n",
        "            window_behavior.append('Down Trend')\n",
        "        else:\n",
        "            window_behavior.append('Mixed Trend')\n",
        "\n",
        "        # Determine behavior in next data based on threshold\n",
        "        if row['Positive Change Percentage in Next Data'] > threshold and row['Average Change in Next Data'] > average:\n",
        "            next_data_behavior.append('Up Trend')\n",
        "        elif row['Negative Change Percentage in Next Data'] > threshold and row['Average Change in Next Data'] < (- average) :\n",
        "            next_data_behavior.append('Down Trend')\n",
        "        else:\n",
        "            next_data_behavior.append('Mixed Trend')\n",
        "\n",
        "    # Create a new DataFrame for cluster behavior\n",
        "    behavior_df = pd.DataFrame({\n",
        "        'Cluster': df.index,\n",
        "        'Window Behavior': window_behavior,\n",
        "        'Next Data Behavior': next_data_behavior,\n",
        "        'Appearance Percentage': df['Appearance Percentage']\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    behavior_df.to_csv('clusters_behavior.csv', index=False)\n",
        "\n",
        "    return behavior_df\n",
        "\n",
        "\n",
        "\n",
        "#group clusters\n",
        "def cluster_groups_nextdata(cluster_behavior_file):\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    if isinstance(cluster_behavior_file, pd.DataFrame):\n",
        "        cluster_behavior_df = cluster_behavior_file\n",
        "    else:\n",
        "        cluster_behavior_df = pd.read_csv(cluster_behavior_file)\n",
        "\n",
        "    # Filter clusters based on behavior in next data\n",
        "    positive_clusters_next = cluster_behavior_df[cluster_behavior_df['Next Data Behavior'] == 'Up Trend']['Cluster'].tolist()\n",
        "    negative_clusters_next = cluster_behavior_df[cluster_behavior_df['Next Data Behavior'] == 'Down Trend']['Cluster'].tolist()\n",
        "\n",
        "    return positive_clusters_next, negative_clusters_next\n",
        "\n",
        "def cluster_groups_window(cluster_behavior_file):\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    if isinstance(cluster_behavior_file, pd.DataFrame):\n",
        "        cluster_behavior_df = cluster_behavior_file\n",
        "    else:\n",
        "        cluster_behavior_df = pd.read_csv(cluster_behavior_file)\n",
        "\n",
        "    # Filter clusters based on behavior in window\n",
        "    positive_clusters_window = cluster_behavior_df[cluster_behavior_df['Window Behavior'] == 'Up Trend']['Cluster'].tolist()\n",
        "    negative_clusters_window = cluster_behavior_df[cluster_behavior_df['Window Behavior'] == 'Down Trend']['Cluster'].tolist()\n",
        "\n",
        "    return positive_clusters_window, negative_clusters_window\n",
        "\n",
        "\n",
        "#get signal\n",
        "def get_signal(window,positive_clusters,negative_clusters,finder):\n",
        "\n",
        "\n",
        "  # Get cluster\n",
        "  cluster = finder.find_cluster_for_record(window)\n",
        "\n",
        "  if cluster in positive_clusters:\n",
        "    decision = \"buy\"\n",
        "  elif cluster in negative_clusters :\n",
        "    decision = \"sell\"\n",
        "  else :\n",
        "    decision = \"hold\"\n",
        "\n",
        "  return decision , cluster\n",
        "\n",
        "\n",
        "#creating sequence\n",
        "def create_sequence (crypto_data_file,clusters_dataset,row_points,finder):\n",
        "\n",
        "\n",
        "  # Load crypto data\n",
        "  df = pd.read_csv(crypto_data_file)\n",
        "  data = df[\"close\"]\n",
        "\n",
        "\n",
        "\n",
        "  # Load or initialize last index processed\n",
        "  try:\n",
        "      with open('last_index_sequence.txt', 'r') as file:\n",
        "          last_index = int(file.read())\n",
        "  except FileNotFoundError:\n",
        "      last_index = 24  # Start from the beginning if last_index.txt does not exist\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize or load result DataFrame\n",
        "  try:\n",
        "      result = pd.read_csv('sequence.csv')\n",
        "  except FileNotFoundError:\n",
        "      result = pd.DataFrame()\n",
        "\n",
        "\n",
        "  # Get number of clusters\n",
        "  num = calculate_unique_clusters(clusters_dataset)\n",
        "\n",
        "\n",
        "  # Start processing from the last index\n",
        "  for i in range(last_index, len(data)):\n",
        "      # Window\n",
        "      window = data[i - 24:i].values\n",
        "\n",
        "      # Get cluster\n",
        "      cluster = finder.find_cluster_for_record(window)\n",
        "\n",
        "      # New record to the dataframe\n",
        "      new_records = {\n",
        "          'cluster': [cluster],\n",
        "      }\n",
        "\n",
        "      #print(cluster)\n",
        "\n",
        "      result = pd.concat([result, pd.DataFrame(new_records)], ignore_index=True)\n",
        "\n",
        "      # Save last index processed\n",
        "      with open('last_index_sequence.txt', 'w') as file:\n",
        "          file.write(str(i))\n",
        "\n",
        "      # Save progress after processing each record\n",
        "      result.to_csv('sequence.csv', index=False)\n",
        "\n",
        "\n",
        "  delete('last_index_sequence.txt')\n",
        "  print(\"Processing complete, sequence file created\")\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "#behavior chart\n",
        "def plot_behavior_chart(cluster_behavior_file):\n",
        "\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    if isinstance(cluster_behavior_file, str):\n",
        "        cluster_behavior_df = pd.read_csv(cluster_behavior_file)\n",
        "    else:\n",
        "        cluster_behavior_df = cluster_behavior_file\n",
        "\n",
        "    # Read the clusters behavior CSV file into a DataFrame\n",
        "    cluster_behavior_df = pd.read_csv(cluster_behavior_file)\n",
        "\n",
        "    # Separate window and next data behavior\n",
        "    window_behavior_df = cluster_behavior_df[['Window Behavior', 'Appearance Percentage']]\n",
        "    next_data_behavior_df = cluster_behavior_df[['Next Data Behavior', 'Appearance Percentage']]\n",
        "\n",
        "    # Calculate the total appearance for each behavior in window and next data\n",
        "    window_up_trend = window_behavior_df[window_behavior_df['Window Behavior'] == 'Up Trend']['Appearance Percentage'].sum()\n",
        "    window_down_trend = window_behavior_df[window_behavior_df['Window Behavior'] == 'Down Trend']['Appearance Percentage'].sum()\n",
        "    window_mixed_trend = window_behavior_df[window_behavior_df['Window Behavior'] == 'Mixed Trend']['Appearance Percentage'].sum()\n",
        "\n",
        "    next_data_up_trend = next_data_behavior_df[next_data_behavior_df['Next Data Behavior'] == 'Up Trend']['Appearance Percentage'].sum()\n",
        "    next_data_down_trend = next_data_behavior_df[next_data_behavior_df['Next Data Behavior'] == 'Down Trend']['Appearance Percentage'].sum()\n",
        "    next_data_mixed_trend = next_data_behavior_df[next_data_behavior_df['Next Data Behavior'] == 'Mixed Trend']['Appearance Percentage'].sum()\n",
        "\n",
        "    # Plot pie chart for window behavior\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    labels = ['Up Trend', 'Down Trend', 'Mixed Trend']\n",
        "    sizes = [window_up_trend, window_down_trend, window_mixed_trend]\n",
        "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "    plt.title('Window Behavior')\n",
        "\n",
        "    # Plot pie chart for next data behavior\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sizes = [next_data_up_trend, next_data_down_trend, next_data_mixed_trend]\n",
        "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "    plt.title('Next Data Behavior')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#to delete unneeded files\n",
        "\n",
        "def delete_files(items):\n",
        "    for item in items:\n",
        "        if os.path.exists(item):\n",
        "            if os.path.isfile(item):\n",
        "                os.remove(item)\n",
        "                print(f\"File '{item}' deleted successfully.\")\n",
        "            elif os.path.isdir(item):\n",
        "                delete_files_in_directory(item)\n",
        "                os.rmdir(item)\n",
        "                print(f\"Directory '{item}' deleted successfully.\")\n",
        "            else:\n",
        "                print(f\"'{item}' is neither a file nor a directory.\")\n",
        "        else:\n",
        "            print(f\"'{item}' does not exist.\")\n",
        "\n",
        "def delete_files_in_directory(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            os.remove(file_path)\n",
        "            print(f\"File '{file_path}' deleted successfully.\")\n",
        "\n",
        "\n",
        "#check folder\n",
        "\n",
        "def folder_check(path):\n",
        "  if os.path.exists(path):\n",
        "      return True\n",
        "  else:\n",
        "      return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#backup data\n",
        "\n",
        "def create_backup_zip(file_list, zip_file_name):\n",
        "    # Create a ZipFile object in write mode\n",
        "    with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "        # Iterate through the list of files and directories\n",
        "        for item in file_list:\n",
        "            # Check if the item exists\n",
        "            if os.path.exists(item):\n",
        "                # If the item is a directory, add its contents recursively\n",
        "                if os.path.isdir(item):\n",
        "                    for foldername, subfolders, filenames in os.walk(item):\n",
        "                        for filename in filenames:\n",
        "                            file_path = os.path.join(foldername, filename)\n",
        "                            zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(item)))\n",
        "                else:\n",
        "                    # If the item is a file, add it directly\n",
        "                    zipf.write(item, os.path.basename(item))\n",
        "            else:\n",
        "                print(f\"Warning: '{item}' not found.\")\n",
        "\n",
        "    print(f\"Zip file '{zip_file_name}' created successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "#extract sequence\n",
        "def extract_sequence(data):\n",
        "    # If data is a DataFrame, directly extract the 'cluster' column\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        return data['cluster'].tolist()\n",
        "    # If data is a CSV file path, read the CSV file and then extract the 'cluster' column\n",
        "    elif isinstance(data, str) and data.endswith('.csv'):\n",
        "        df = pd.read_csv(data)\n",
        "        return df['cluster'].tolist()\n",
        "    else:\n",
        "        raise ValueError(\"Input data must be a DataFrame or a CSV file path ending with '.csv'\")\n",
        "\n",
        "\n",
        "#info\n",
        "def get_cluster_info(dataframe, cluster_name):\n",
        "\n",
        "\n",
        "    # If data is a CSV file path, read the CSV file and then extract the 'cluster' column\n",
        "    if isinstance(dataframe, str) and data.endswith('.csv'):\n",
        "       dataframe = pd.read_csv(dataframe)\n",
        "\n",
        "    # Search for the cluster name in the 'Cluster' column\n",
        "    cluster_row = dataframe[dataframe['Cluster'] == cluster_name]\n",
        "\n",
        "    # Check if the cluster exists\n",
        "    if len(cluster_row) == 0:\n",
        "        print(f\"Cluster '{cluster_name}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Extract information from the row\n",
        "    window_behavior = cluster_row['Window Behavior'].iloc[0]\n",
        "    next_data_behavior = cluster_row['Next Data Behavior'].iloc[0]\n",
        "    appearance_percentage = cluster_row['Appearance Percentage'].iloc[0]\n",
        "\n",
        "    return window_behavior, next_data_behavior, appearance_percentage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def search_cluster(dataframe, cluster_id):\n",
        "    # Filter the DataFrame based on the cluster ID\n",
        "    cluster_data = dataframe[dataframe.index.str.startswith(cluster_id)]\n",
        "\n",
        "    # Return the columns of the cluster\n",
        "    return cluster_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def print_loading_bar(progress, total, bar_length=50):\n",
        "    \"\"\"\n",
        "    Print a loading bar based on the progress and total steps.\n",
        "\n",
        "    Parameters:\n",
        "        progress (int): The current progress.\n",
        "        total (int): The total steps.\n",
        "        bar_length (int): The length of the loading bar.\n",
        "    \"\"\"\n",
        "    percent = progress / total\n",
        "    arrow = '>' * int(round(percent * bar_length) - 1)\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "    sys.stdout.write(f'\\r[{arrow + spaces}] {int(percent * 100)}%')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#classes\n",
        "\n",
        "\n",
        "#creating tree from sequence\n",
        "class FBTree:\n",
        "    def __init__(self):\n",
        "        self.tree = {}\n",
        "\n",
        "    def build_tree(self, sequence):\n",
        "\n",
        "        if isinstance(sequence, list):\n",
        "          print(\"sequence loaded\")\n",
        "        else:\n",
        "          print(\"converting dataframe into sequence\")\n",
        "\n",
        "          try :\n",
        "            sequence = pd.read_csv(sequence)\n",
        "            sequence = sequence['cluster'].tolist()\n",
        "          except:\n",
        "            try :\n",
        "              sequence = sequence['cluster'].tolist()\n",
        "            except :\n",
        "              print(\"error converting dataframe to sequence, pleasae check the datafram\")\n",
        "\n",
        "\n",
        "        for i in range(len(sequence) - 1):\n",
        "            current = self.tree\n",
        "            current = current.setdefault(sequence[i], {})\n",
        "            current.setdefault(sequence[i+1], 0)\n",
        "            current[sequence[i+1]] += 1\n",
        "\n",
        "    def save_tree(self, filename):\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(self.tree, file)\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def load_tree(self, filename):\n",
        "        with open(filename, 'r') as file:\n",
        "            self.tree = json.load(file)\n",
        "\n",
        "    def get_next_steps(self, data):\n",
        "        if data not in self.tree:\n",
        "            return {}\n",
        "        current = self.tree[data]\n",
        "        total_count = sum(current.values())\n",
        "        next_steps = {key: count / total_count for key, count in current.items()}\n",
        "\n",
        "        df = pd.DataFrame(list(next_steps.items()), columns=['name', 'percentage'])\n",
        "        df['percentage'] *= 100\n",
        "        return df\n",
        "\n",
        "    def display_tree(self):\n",
        "        self._display_tree_recursive(self.tree, '', '')\n",
        "\n",
        "    def _display_tree_recursive(self, node, prefix, last_prefix):\n",
        "        if isinstance(node, int):  # Handle case where node is an integer value\n",
        "            return\n",
        "        keys = list(node.keys())\n",
        "        for i, key in enumerate(keys):\n",
        "            is_last = i == len(keys) - 1\n",
        "            print(prefix + (\" \" if is_last else \" \") + str(key))\n",
        "            new_prefix = last_prefix + (\"    \" if is_last else \"   \")\n",
        "            new_last_prefix = last_prefix + (\"    \" if is_last else \"   \")\n",
        "            self._display_tree_recursive(node[key], new_prefix, new_last_prefix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ClusterFinder:\n",
        "    def __init__(self, data_file, num_clusters, save_file):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.features = self.data.iloc[:, :-1].values\n",
        "        self.clusters = self.data['cluster'].values\n",
        "        self.tree = KDTree(self.features)\n",
        "        self.save_file = save_file\n",
        "\n",
        "    def save_tree(self):\n",
        "        with open(self.save_file, 'wb') as f:\n",
        "            pickle.dump(self.tree, f)\n",
        "\n",
        "    def load_tree(self,file_name):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            self.tree = pickle.load(f)\n",
        "\n",
        "\n",
        "    def find_cluster_for_record(self, record):\n",
        "\n",
        "        #get points\n",
        "        data = pips(record, row_points, 2)\n",
        "\n",
        "        #find cluster\n",
        "        _, indices = self.tree.query([data], k=1)\n",
        "        closest_cluster = self.clusters[indices[0][0]]\n",
        "        return closest_cluster\n",
        "\n",
        "    def find_cluster_for_pips(self, record):\n",
        "\n",
        "        _, indices = self.tree.query([record], k=1)\n",
        "        closest_cluster = self.clusters[indices[0][0]]\n",
        "        return closest_cluster\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main class\n",
        "\n",
        "class cluster_analysis:\n",
        "\n",
        "    def __init__(self,name,data_file,row_points,window_size,step_size,range_main,range_secondary,threshold = 70,average=0.5,pom=-1 ):\n",
        "        #name\n",
        "        self.name = name\n",
        "        #variable\n",
        "        self.data_file = data_file\n",
        "        self.row_points = row_points\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.range_main = range_main\n",
        "        self.range_secondary = range_secondary\n",
        "        self.threshold = threshold\n",
        "        self.ave_threshold = average\n",
        "        self.pom = pom\n",
        "        #defult variables\n",
        "        self.clusters_folder= \"secondary_clusters\"\n",
        "        self.clusters_dataset = \"clusters_dataset.csv\"\n",
        "        self.future_dataset = \"future_result.csv\"\n",
        "        self.treefile = \"clusters_tree.json\"\n",
        "        self.finder_file = f\"{self.name}_cluster_finder.pkl\"\n",
        "        self.behavior_file = \"clusters_behavior.csv\"\n",
        "        #variable init\n",
        "        self.num = 0\n",
        "        self.cluster_finder = \"\"\n",
        "        self.future_dataset_data = \"\"\n",
        "        self.clusters_analysis = \"\"\n",
        "        self.clusters_behavior = \"\"\n",
        "        self.clusters_sequence = []\n",
        "        self.fb_tree = \"\"\n",
        "        #output\n",
        "        self.positive_clusters_next = []\n",
        "        self.negative_clusters_next = []\n",
        "        self.positive_clusters_window = []\n",
        "        self.negative_clusters_window = []\n",
        "        #time\n",
        "        self.train_time = 0\n",
        "        #backtest results\n",
        "        self.backtest_results = 0\n",
        "\n",
        "      # to save instance _________________________________________________\n",
        "    def save(self):\n",
        "        # Save the instance to a file\n",
        "        with open(f\"{self.name}.pkl\", \"wb\") as file:\n",
        "            pickle.dump(self, file)\n",
        "        print(f\"{self.name} saved successfully\" )\n",
        "\n",
        "    def load(self,obj_file):\n",
        "        # Load the instance from the file\n",
        "        with open(obj_file, \"rb\") as file:\n",
        "            loaded_instance = pickle.load(file)\n",
        "        self.__dict__.update(loaded_instance.__dict__)\n",
        "        print(f\"{obj_file} loaded successfully\" )\n",
        "\n",
        "    # core functions  _________________________________________________\n",
        "    def train(self):\n",
        "        #get time\n",
        "        start_time = time.time()\n",
        "        #to cluster data\n",
        "        self.num = pattern_clustering(self.data_file,self.row_points,self.window_size,self.step_size,self.range_main,self.range_secondary)\n",
        "        print(\"clutsering done successfully\")\n",
        "        # create the dataframe for training the model for secondary clusters:\n",
        "        combine_secondary_clusters(self.clusters_folder, self.clusters_dataset)\n",
        "        print(\"dataset created successfully\")\n",
        "        # Get number of clusters\n",
        "        self.num = calculate_unique_clusters(self.clusters_dataset)\n",
        "        # create instance from ClusterFinder\n",
        "        self.cluster_finder = ClusterFinder(self.clusters_dataset, self.num, self.finder_file)\n",
        "        print(\"cluster finder created successfully\")\n",
        "        # check the future results for every cluster\n",
        "        print(\"backtest started\")\n",
        "        self.future_dataset_data = create_future_dataset(self.data_file,self.cluster_finder,self.num)\n",
        "        print(\"backtesting done successfully\")\n",
        "        # create clusters analysis\n",
        "        self.clusters_analysis = create_clusters_analysis(self.future_dataset,self.pom,self.window_size)\n",
        "        print(\"backtest analysed successfully\")\n",
        "        #get cluster behavior\n",
        "        self.clusters_behavior = get_clusters_behavior(self.threshold,self.ave_threshold, self.clusters_analysis)\n",
        "        print(\"behaviors extracted successfully\")\n",
        "        #group clusters for strategy\n",
        "        self.positive_clusters_next, self.negative_clusters_next = cluster_groups_nextdata(self.clusters_behavior)\n",
        "        self.positive_clusters_window, self.negative_clusters_window = cluster_groups_window(self.clusters_behavior)\n",
        "        print(\"clusters groups created successfully\")\n",
        "        #create clusters sequence\n",
        "        self.clusters_sequence = extract_sequence(self.future_dataset_data)\n",
        "        #creating the fb tree of clusters\n",
        "        self.fb_tree = FBTree()\n",
        "        self.fb_tree.build_tree(self.clusters_sequence)\n",
        "        print(\"sequence fb tree created successfully\")\n",
        "        #print\n",
        "        print(\"**********training done successfully**********\")\n",
        "        #get time\n",
        "        end_time = time.time()\n",
        "        self.train_time = end_time - start_time\n",
        "        print(f\"train time : {self.train_time/60} min\")\n",
        "\n",
        "    def get_signals(self,records):\n",
        "        #to use into the backtest\n",
        "        signal , current_cluster = get_signal(records,self.positive_clusters_next,self.negative_clusters_next,self.cluster_finder)\n",
        "        #print results\n",
        "        print(f\"signal : {signal} , current cluster {current_cluster} \")\n",
        "        #return\n",
        "        return signal , current_cluster\n",
        "\n",
        "\n",
        "    def get_info(self,cluster):\n",
        "      #get cluster info\n",
        "      info = search_cluster(self.clusters_analysis,cluster)\n",
        "      return info\n",
        "\n",
        "\n",
        "    def clean_files(self):\n",
        "        #delete files\n",
        "        file_list=[\"last_index.txt\",\"clusters\",\"clusters_analysis.csv\" ,  \"dataset.csv\" , \"future_result.csv\" ,\"clusters_dataset.csv\",\"clusters_behavior.csv\",\"secondary_clusters\" ]\n",
        "        delete_files(file_list)\n",
        "\n",
        "\n",
        "    def plot_behavior(self):\n",
        "        #plot behavior\n",
        "        plot_behavior_chart(self.behavior_file)\n",
        "\n",
        "    def save_cluster_finder(self):\n",
        "        # Save the KDTree to a file\n",
        "        self.cluster_finder.save_tree()\n",
        "        #print\n",
        "        print(\"finder saved succesfully\")\n",
        "\n",
        "    def load_cluster_finder(self,file_name):\n",
        "        #to load cluster finder\n",
        "        self.cluster_finder.load_tree(file_name)\n",
        "        print(f\"{file_name} loaded successfully\" )\n",
        "\n",
        "    def plot_patterns(self):\n",
        "        #plot patterns\n",
        "        plot_clusters(self.clusters_folder,self.row_points)\n",
        "\n",
        "\n",
        "    def save_patterns_plots(self):\n",
        "        #save plots\n",
        "        save_cluster_plots(self.clusters_folder,self.row_points)\n",
        "        print(f\"plots saved successfully\" )\n",
        "\n",
        "    def continue_future_test(self):\n",
        "        # check the future results for every cluster\n",
        "        self.future_dataset_data = create_future_dataset(self.data_file,self.cluster_finder,self.num)\n",
        "        # create clusters analysis\n",
        "        self.clusters_analysis = create_clusters_analysis(self.future_dataset)\n",
        "        #get cluster behavior\n",
        "        self.clusters_behavior = get_clusters_behavior(self.threshold,self.ave_threshold, self.clusters_analysis)\n",
        "        #group clusters for strategy\n",
        "        self.positive_clusters_next, self.negative_clusters_next = cluster_groups_nextdata(self.clusters_behavior)\n",
        "        self.positive_clusters_window, self.negative_clusters_window = cluster_groups_window(self.clusters_behavior)\n",
        "        #create clusters sequence\n",
        "        self.clusters_sequence = extract_sequence(self.future_dataset_data)\n",
        "        #creating the fb tree of clusters\n",
        "        self.fb_tree = FBTree()\n",
        "        self.fb_tree.build_tree(self.clusters_sequence)\n",
        "        print(\"sequence fb tree created successfully\")\n",
        "        #print\n",
        "        print(\"**********training done successfully**********\")\n",
        "\n",
        "\n",
        "    def backup(self):\n",
        "      file_list = [\"last_index.txt\", \"clusters\", \"clusters_analysis.csv\", \"dataset.csv\", \"future_result.csv\", \"clusters_dataset.csv\", \"clusters_behavior.csv\", \"secondary_clusters\"]\n",
        "      zip_file_name = f\"{self.name}_backup.zip\"\n",
        "      create_backup_zip(file_list, zip_file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #tree functions _________________________________________________\n",
        "    def save_tree(self):\n",
        "      self.fb_tree.save_tree(f\"{self.name}_sequence_tree.json\")\n",
        "      print(f\"{self.name}_sequence_tree saved successfully\" )\n",
        "\n",
        "    def load_tree(self,file_name):\n",
        "      self.fb_tree.load_tree(file_name)\n",
        "      print(f\"{file_name}_sequence_tree loaded successfully\" )\n",
        "\n",
        "    def next_cluster(self,cluster):\n",
        "      df = self.fb_tree.get_next_steps(cluster)\n",
        "      df[['window_behavior', 'next_data_behavior', 'appearance_percentage']] = df['name'].apply(lambda x: pd.Series(get_cluster_info(self.clusters_behavior, x)))\n",
        "      return df\n",
        "\n",
        "    def display_tree(self):\n",
        "      self.fb_tree.display_tree()\n",
        "\n",
        "\n",
        "    #backtest functions _________________________________________________\n",
        "    def set_new(self,ave,threshold):\n",
        "        #setting variables\n",
        "        self.threshold = threshold\n",
        "        self.ave_threshold = ave\n",
        "        #get cluster behavior\n",
        "        self.clusters_behavior = get_clusters_behavior(self.threshold,self.ave_threshold, self.clusters_analysis)\n",
        "        #group clusters for strategy\n",
        "        self.positive_clusters_next, self.negative_clusters_next = cluster_groups_nextdata(self.clusters_behavior)\n",
        "        self.positive_clusters_window, self.negative_clusters_window = cluster_groups_window(self.clusters_behavior)\n",
        "        #create clusters sequence\n",
        "        self.clusters_sequence = extract_sequence(self.future_dataset_data)\n",
        "        #creating the fb tree of clusters\n",
        "        self.fb_tree = FBTree()\n",
        "        self.fb_tree.build_tree(self.clusters_sequence)\n",
        "        #print\n",
        "        print(\"**********changing variables done successfully**********\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxfEF8GR4-jG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#documentation"
      ],
      "metadata": {
        "id": "EjRPSkMxDQP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alo alo alo ,,,, Ahmed Sleem here, just finished up version one of this module. It's April 9th, 2024, 9:52 am. Oh, and heads up, I haven't slept since yesterday at 10 am. So, there might be some hiccups to sort out in the next versions. But hey, we'll iron those out as we go.\n",
        "\n",
        "Overview:\n",
        "This module helps you analyze historical data for a particular currency, giving you insights into its clustering patterns. With this analysis, you can do a bunch of stufffrom making decisions to plotting patterns or understanding its behaviors through numbers. Just make sure to read the comments carefully!"
      ],
      "metadata": {
        "id": "yQp4UXRG135_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first you need to your analysis object instance\n",
        "\n",
        "#the name of the object\n",
        "name = \"sol_analysis\"\n",
        "#the historical data of the currency\n",
        "data_file = \"sol.csv\"\n",
        "#the number of point that will be in every pattern after simplification (you can fine tune it, defult = 5)\n",
        "row_points = 5\n",
        "#the window size for each pattern before simplification (you can fine tune it, defult = 24)\n",
        "window_size = 24\n",
        "#the steps to skip between every pattern and the next one (you can fine tune it, defult = 10)\n",
        "step_size = 5\n",
        "#the object will detect the most suitable number of cluster inside the range you give here (you can fine tune it, defult = (25,45))\n",
        "range_main = range(25,40)\n",
        "#for every pattern the object will find internal variations inside it , this is the range of vaiation for every pattern and the object will change the range dynamicly and find the most suitable number of clusters insde the range\n",
        "range_secondary = range(10,20)\n",
        "#if the percentage of possitve trend after some pattern is greater than the threshold it will considerd up trend pattern and the opposite for down trend\n",
        "threshold = 70\n",
        "#the minimum average of up trend sizes of the clusters for up trend clusters and with negative for down trend ones\n",
        "min_average = 1\n",
        "#the point of measuring the change after the cluster detecter (length of trade)\n",
        "pom = 10\n",
        "\n",
        "#creating the instance\n",
        "sol_analysis = cluster_analysis(name,data_file,row_points,window_size,step_size,range_main,range_secondary,threshold,min_average,pom)"
      ],
      "metadata": {
        "id": "kkW5StmwDRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main functionalities :\n",
        "\n",
        "#train the system\n",
        "sol_analysis.train()\n",
        "\n",
        "#if you stoped the training process in the backtesting phase (because it might need sometime to complete) you can continue from the points you stopped in\n",
        "sol_analysis.continue_future_test()\n",
        "\n",
        "#to plot the discovered patterns after training\n",
        "sol_analysis.plot_patterns()\n",
        "\n",
        "#to save patterns as images in plots folder\n",
        "sol_analysis.save_patterns_plots()\n",
        "\n",
        "#to plot the pie chart that represent the percentage of up trend patterns to down trend patterns to mixed trend patterns in the patterns discovered from the historical data\n",
        "#it is important to predict the trading frequency depend on this analysis\n",
        "sol_analysis.plot_behavior()\n",
        "\n",
        "#to save the instance to load it later and dont have to start train it again\n",
        "sol_analysis.save()\n",
        "\n",
        "#to delete unwanted files after training and saving the instance you might need to delete unwanted files that created due to training\n",
        "sol_analysis.clean_files()\n",
        "\n",
        "#to get signal depend on the result of the analysis simply follow the this example\n",
        "window = [\n",
        "    107.57, 107.42, 107.76, 107.75, 107.42, 108.16, 108.09, 108.42, 108.85,\n",
        "    109.39, 108.87, 108.58, 108.31, 108.07, 107.99, 107.72, 107.39, 107.96,\n",
        "    107.69, 107.33, 107.1, 106.85, 106.65, 107.02\n",
        "]\n",
        "signal , cluster = sol_analysis.get_signals(window)\n",
        "\n",
        "#get the analysis of certain cluster\n",
        "info = sol_analysis.get_info(cluster)\n",
        "#max up trend\n",
        "max = float(info[\"Max Change in Next Data\"].values[0])\n",
        "#max down trend\n",
        "min = float(info[\"Min Change in Next Data\"].values[0])\n",
        "\n",
        "\n",
        "#to know the future possiblities you should this function , depend on an internal fb tree inside the instance it should tells you the patterns that most likely to happen after the current one and what is its behaviors and the percentage of each of them to be the next one\n",
        "possible_clusters = sol_analysis.next_cluster(cluster)\n",
        "print(possible_clusters)\n",
        "\n",
        "#to load any saved pretrained instances and work with it directly\n",
        "sol_analysis.load(\"sol_analysis.pkl\")\n",
        "\n",
        "\n",
        "#backtest data\n",
        "data_test = \"sol test.csv\"\n",
        "#to backtest the strategy based on the result\n",
        "sol_analysis.backtest(data_test)\n",
        "\n",
        "\n",
        "#to change the average and threshold after training\n",
        "new_threshold = 60\n",
        "new_average = 0.5\n",
        "\n",
        "sol_analysis.set_new(new_average , new_threshold)\n",
        "\n"
      ],
      "metadata": {
        "id": "GBpjIxYPFA8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#additional functionalities :\n",
        "\n",
        "#to save only the finder object that can detect patterns without any other data\n",
        "sol_analysis.save_cluster_finder()\n",
        "\n",
        "#backup analysis files like analysis csvs that secure trianing process\n",
        "sol_analysis.backup()\n",
        "\n",
        "\n",
        "#to display the fb tree that represent the sequence of patterns that happen after each other\n",
        "sol_analysis.display_tree()\n",
        "\n",
        "\n",
        "#to save the fb tree\n",
        "sol_analysis.save_tree()\n",
        "\n",
        "\n",
        "#to load saved fb tree\n",
        "sol_analysis.load_tree()\n",
        "\n",
        "\n",
        "#to load saved finder file\n",
        "sol_analysis.load_cluster_finder(\"sol_analysis_cluster_finder.pkl\")\n"
      ],
      "metadata": {
        "id": "5OeJGivp9D-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#step by step"
      ],
      "metadata": {
        "id": "wsgp9LFk_3ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first you need to your analysis object instance\n",
        "\n",
        "#the name of the object\n",
        "name = \"sol_analysis\"\n",
        "#the historical data of the currency\n",
        "data_file = \"sol.csv\"\n",
        "#the number of point that will be in every pattern after simplification (you can fine tune it, defult = 5)\n",
        "row_points = 5\n",
        "#the window size for each pattern before simplification (you can fine tune it, defult = 24)\n",
        "window_size = 24\n",
        "#the steps to skip between every pattern and the next one (you can fine tune it, defult = 10)\n",
        "step_size = 5\n",
        "#the object will detect the most suitable number of cluster inside the range you give here (you can fine tune it, defult = (25,45))\n",
        "range_main = range(25,40)\n",
        "#for every pattern the object will find internal variations inside it , this is the range of vaiation for every pattern and the object will change the range dynamicly and find the most suitable number of clusters insde the range\n",
        "range_secondary = range(10,20)\n",
        "#if the percentage of possitve trend after some pattern is greater than the threshold it will considerd up trend pattern and the opposite for down trend\n",
        "threshold = 70\n",
        "#the minimum average of up trend sizes of the clusters for up trend clusters and with negative for down trend ones\n",
        "min_average = 1\n",
        "#the point of measuring the change after the cluster detecter (length of trade)\n",
        "pom = 10\n",
        "\n",
        "\n",
        "#backtest data\n",
        "data_test = \"sol test.csv\"\n",
        "\n",
        "#creating the instance\n",
        "sol_analysis = cluster_analysis(name,data_file,row_points,window_size,step_size,range_main,range_secondary,threshold,min_average,pom)"
      ],
      "metadata": {
        "id": "SpcsIQRKJGXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to train the system\n",
        "sol_analysis.train()"
      ],
      "metadata": {
        "id": "qXrR7Mj28qLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if the train doesnt finished in one session you can contine from the point you stopped in\n",
        "sol_analysis.continue_future_test()"
      ],
      "metadata": {
        "id": "0vRhOajJWMT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to backtest the strategy based on the result\n",
        "sol_analysis.backtest(data_test)"
      ],
      "metadata": {
        "id": "OOFcbxZUG3AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to plot clusters\n",
        "sol_analysis.plot_patterns()"
      ],
      "metadata": {
        "id": "0wYrEtcyRMoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to save patterns as images\n",
        "sol_analysis.save_patterns_plots()"
      ],
      "metadata": {
        "id": "XGRUW8WyWHDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to plot the pie chart\n",
        "sol_analysis.plot_behavior()"
      ],
      "metadata": {
        "id": "r7rKbTvlV6QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to change the average and threshold after training\n",
        "new_threshold = 60\n",
        "new_average = 0.5\n",
        "\n",
        "sol_analysis.set_new(new_average , new_threshold)"
      ],
      "metadata": {
        "id": "xZPqQIu5LCgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to save finder\n",
        "sol_analysis.save_cluster_finder()"
      ],
      "metadata": {
        "id": "ckPrUxKmRelp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to save analysis instance\n",
        "sol_analysis.save()"
      ],
      "metadata": {
        "id": "4oidDqNGOHe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backup analysis files\n",
        "sol_analysis.backup()"
      ],
      "metadata": {
        "id": "pe9-lkz3OyTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to display tree\n",
        "sol_analysis.display_tree()"
      ],
      "metadata": {
        "id": "kg5yV9aYqOeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to save tree\n",
        "sol_analysis.save_tree()"
      ],
      "metadata": {
        "id": "Y2TkkkOrqW2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to load tree\n",
        "sol_analysis.load_tree()"
      ],
      "metadata": {
        "id": "PXnQhXTHqcVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to delete unwanted files\n",
        "sol_analysis.clean_files()"
      ],
      "metadata": {
        "id": "_gYf_W4dQPrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to get signals\n",
        "window = [\n",
        "    107.57, 107.42, 107.76, 107.75, 107.42, 108.16, 108.09, 108.42, 108.85,\n",
        "    109.39, 108.87, 108.58, 108.31, 108.07, 107.99, 107.72, 107.39, 107.96,\n",
        "    107.69, 107.33, 107.1, 106.85, 106.65, 107.02\n",
        "]\n",
        "signal , cluster = sol_analysis.get_signals(window)"
      ],
      "metadata": {
        "id": "sI9nfeHlOKX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the analysis of certain cluster\n",
        "info = sol_analysis.get_info(cluster)\n",
        "\n",
        "\n",
        "#max up trend\n",
        "take_profit = float(info[\"Max Change in Next Data\"].values[0])\n",
        "#max down trend\n",
        "stop_loss = float(info[\"Min Change in Next Data\"].values[0])\n",
        "\n",
        "print(take_profit)\n",
        "print(stop_loss)"
      ],
      "metadata": {
        "id": "1_0QyoYL2hsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ccacd6-1809-45a6-b2a4-e22c9835acb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.099999999999994\n",
            "-4.549999999999997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to get next possible clusters\n",
        "possible_clusters = sol_analysis.next_cluster(cluster)\n",
        "print(possible_clusters)\n",
        "\n"
      ],
      "metadata": {
        "id": "l7w_cWNGqibb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to load old analysis file\n",
        "sol_analysis.load(\"sol_analysis.pkl\")"
      ],
      "metadata": {
        "id": "6NeSI1PZV13S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to load old finder file\n",
        "sol_analysis.load_cluster_finder(\"sol_analysis_cluster_finder.pkl\")"
      ],
      "metadata": {
        "id": "s0E27Q5GQVV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run"
      ],
      "metadata": {
        "id": "z09dEPdnHrgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for experment\n",
        "\n",
        "name = \"sol_analysis\"\n",
        "data_file = \"TRAIN.csv\" #must have close with small c\n",
        "data_test = \"TEST.csv\" #  must be a pandas.DataFrame with columns 'Open', 'High', 'Low', 'Close', and (optionally) 'Volume'\n",
        "\n",
        "\n",
        "row_points = 5\n",
        "window_size = 24\n",
        "step_size = 5\n",
        "range_main = range(45,55)\n",
        "range_secondary = range(30,50)\n",
        "threshold = 70\n",
        "min_average = 0.1\n",
        "pom = 24\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#creating the instance\n",
        "sol_analysis = cluster_analysis(name,data_file,row_points,window_size,step_size,range_main,range_secondary,threshold,min_average,pom)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sol_analysis.train()\n",
        "sol_analysis.save()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vyyWrDgPHsu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sol_analysis.continue_future_test()"
      ],
      "metadata": {
        "id": "FLNQ9WjYYFX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to plot the pie chart\n",
        "sol_analysis.plot_behavior()"
      ],
      "metadata": {
        "id": "_3T_106iIaxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to plot clusters\n",
        "sol_analysis.plot_patterns()"
      ],
      "metadata": {
        "id": "VSfT_0XTIxfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to backup all model files\n",
        "sol_analysis.backup()"
      ],
      "metadata": {
        "id": "cnk1JGurvzuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to clean the directory\n",
        "sol_analysis.clean_files()"
      ],
      "metadata": {
        "id": "EGhXsd_21IcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to change the average and threshold after training\n",
        "new_threshold = 60\n",
        "new_average = 0.5\n",
        "\n",
        "sol_analysis.set_new(new_average , new_threshold)"
      ],
      "metadata": {
        "id": "kInLl42b__gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#backtesting"
      ],
      "metadata": {
        "id": "PQJtrdTUBWWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover\n",
        "\n",
        "\n",
        "\n",
        "class str(Strategy):\n",
        "\n",
        "    window_len = 24\n",
        "\n",
        "    def init(self):\n",
        "        close = self.data.Close\n",
        "\n",
        "\n",
        "    def next(self):\n",
        "\n",
        "        #signal\n",
        "        if len(self.data) >= self.window_len:\n",
        "          window = np.array(self.data.Close[-self.window_len:])\n",
        "          signal , cluster = sol_analysis.get_signals(window)\n",
        "\n",
        "          if \"buy\" in signal:\n",
        "              self.buy()\n",
        "          elif \"sell\" in signal:\n",
        "              self.sell()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(data_test)\n",
        "\n",
        "bt = Backtest(data, str,\n",
        "              cash=1000, commission=.002,\n",
        "              exclusive_orders=True)\n",
        "\n",
        "output = bt.run()\n",
        "bt.plot()\n",
        "print(output)"
      ],
      "metadata": {
        "id": "1eGLsyIbBVbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with stop loss and take profit\n",
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover\n",
        "from backtesting.lib import TrailingStrategy\n",
        "\n",
        "\n",
        "\n",
        "class str(TrailingStrategy):\n",
        "\n",
        "    window_len = 24\n",
        "\n",
        "    trailing = 2\n",
        "    tpr = 0.2\n",
        "\n",
        "    def init(self):\n",
        "        close = self.data.Close\n",
        "\n",
        "\n",
        "    def next(self):\n",
        "\n",
        "        #signal\n",
        "        if len(self.data) >= self.window_len:\n",
        "          window = np.array(self.data.Close[-self.window_len:])\n",
        "          signal , cluster = sol_analysis.get_signals(window)\n",
        "\n",
        "          self.set_trailing_sl((self.trailing))\n",
        "\n",
        "\n",
        "          if \"buy\" in signal:\n",
        "\n",
        "              self.tp = self.data.Close *(1+self.tpr/100)\n",
        "              self.buy(tp = self.tp)\n",
        "          elif \"sell\" in signal:\n",
        "              self.sell()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(data_test)\n",
        "\n",
        "bt = Backtest(data, str,\n",
        "              cash=1000, commission=.002,\n",
        "              exclusive_orders=True)\n",
        "\n",
        "output = bt.run()\n",
        "bt.plot()\n",
        "print(output)"
      ],
      "metadata": {
        "id": "nOGxxk1IMBdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#whats new"
      ],
      "metadata": {
        "id": "uM2aZn5Dxs-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ahmed sleem\n",
        "\n",
        "11-april-2024 :\n",
        "*   faster train\n",
        "*   reduced code\n",
        "*   get_info() function\n",
        "*   time calculation (time of train)\n",
        "\n",
        "15-april-2024 :\n",
        "*   custimazed pom (point of measure)\n",
        "\n",
        "16-april-2024:\n",
        "*   add backtest\n",
        "*   fix some issues\n",
        "\n",
        "17-april-2024:\n",
        "*   adding average threshold\n",
        "*   fix sum bugs + fix backtest\n",
        "\n",
        "18-april-2024:\n",
        "*   exact pom calculation\n",
        "\n",
        "19-april-2024:\n",
        "*   fix long name problem\n",
        "*   fix naming (soon)\n",
        "*   calculate possibilities for clusters less than 2 records (soon)\n",
        "*   fix some bugs in backtest\n",
        "\n",
        "20-april-2024:\n",
        "*   reduce training time by 20%\n",
        "*   add set() to reduce the number of trains\n",
        "*   adding stop loss and take profite features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pV3yYqHaxvF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##omar karim\n",
        "\n",
        "17-april-2024:\n",
        "best values til now:\n",
        "*   create our model with parameters = (\n",
        "  row_points = 6\n",
        "  window_size = 24\n",
        "  step_size = 4\n",
        "  range_main = range(40,50)\n",
        "  range_secondary = range(20,40)\n",
        "  threshold = 70\n",
        "  min_average = 0.25\n",
        "  pom = 5\n",
        "  number of main cluster : 39\n",
        "  number of secondary cluster : 281\n",
        ")\n",
        "*   create our model with results = (7.7 up, 6.3 down, 86 mixed)\n",
        "\n",
        "18-april-2024:\n",
        "*   multi-layered system (multible freqencies) -> current focus: short frequency\n",
        "*   backtest and analysis (soon)\n",
        "\n"
      ],
      "metadata": {
        "id": "VDAIbZ7L87pa"
      }
    }
  ]
}